{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cnnproject","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"xS5tIQR4_FWn"},"source":["import tensorflow as tf\n","from keras.datasets import mnist\n","from keras.models import Sequential, load_model\n","from keras.layers import Dense, Flatten, Dropout\n","from tensorflow.keras.utils import normalize\n","from keras.utils import np_utils\n","from keras.layers.convolutional import Conv2D, MaxPooling2D\n","from tensorflow.keras import layers\n","from keras.layers import GaussianNoise\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib.pyplot import figure\n","from google.colab import drive"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K4jnyp6pHWNC"},"source":["# Load Data"]},{"cell_type":"code","metadata":{"id":"NpsBF15u_3aG"},"source":["# Loading the data\n","(training_images, training_labels), (testing_images, testing_labels) = mnist.load_data()\n","\n","# checking if the variables are exactly equal to what is expected\n","assert training_images.shape == (60000, 28, 28)\n","assert testing_images.shape == (10000, 28, 28)\n","assert training_labels.shape == (60000,)\n","assert testing_labels.shape == (10000,)\n","\n","# add the first 4 images to a plot\n","for i in range(4):\n","    plt.subplot(1, 4, (i+1), frameon=False)\n","    plt.title(f'label={training_labels[i]}')\n","    plt.imshow(training_images[i], cmap = plt.get_cmap('binary'))\n","\n","plt.show()\n","\n","# print image 1\n","print('Image 1 array')\n","print(training_images[0])\n","\n","# flatten 3D images into 1D vectors\n","training_images = training_images.reshape((training_images.shape[0], 28, 28, 1))\n","testing_images = testing_images.reshape((testing_images.shape[0], 28, 28, 1))\n","\n","# Convert vector to binary class\n","training_labels_categorical = np_utils.to_categorical(training_labels)\n","testing_labels_categorical = np_utils.to_categorical(testing_labels)\n","\n","# normalize values to scale them between 0-1, makes it easier for model\n","# values are in greyscale so will be between 0 and 255\n","training_images = training_images / 255\n","testing_images = testing_images / 255\n","\n","# add additional images to the training set\n","# def augment_data():\n","#   aug = tf.keras.preprocessing.image.ImageDataGenerator(\n","#   rotation_range=180,\n","#   zoom_range=0.05,\n","#   width_shift_range=0.1,\n","#   height_shift_range=0.1,\n","#   shear_range=0.15,\n","#   horizontal_flip=False,\n","#   fill_mode=\"nearest\")\n","#   return aug"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k_t0UOKAHchY"},"source":["# Create Model"]},{"cell_type":"code","metadata":{"id":"QGJTlL2yEju6"},"source":["# create the base model\n","model = Sequential()\n","model.add(tf.keras.Input(shape=(28, 28, 1)))\n","# add noise to the images\n","model.add(GaussianNoise(0.1))\n","# extracts certain features of the image, eg diagonal or horizontal lines\n","model.add(Conv2D(32, kernel_size=(3, 3) , activation = 'relu', padding='same'))\n","# Reduces dimensions of the image by taking the max value of each area eg 2x2\n","model.add(MaxPooling2D(pool_size=(2, 2), strides=2, padding='valid'))\n","model.add(Dropout(0.5))\n","# second Conv2D layer which will extract more complex features\n","model.add(Conv2D(64, kernel_size=(3, 3) , activation = 'relu', padding='same'))\n","model.add(MaxPooling2D(pool_size=(2, 2), strides=2, padding='valid'))\n","model.add(Dropout(0.5))\n","# third Conv2D layer which will extract more complex features\n","model.add(Conv2D(128, kernel_size=(3, 3) , activation = 'relu', padding='same'))\n","model.add(MaxPooling2D(pool_size=(2, 2), strides=2, padding='valid'))\n","model.add(Dropout(0.5))\n","# convert image into 1D binary class\n","model.add(Flatten())\n","\n","# add fully connected layer. The purpose of this is to add some selection\n","# inteligence to the convulted image\n","model.add(Dense(128, activation='relu'))\n","\n","# converts output into a value that can be used as a probability\n","model.add(Dense(10, activation='softmax'))\n","\n","# TODO : visualise the model\n","\n","#compile model using accuracy to measure model performance\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fnoExeVoAXER","executionInfo":{"status":"ok","timestamp":1636601900887,"user_tz":-780,"elapsed":906,"user":{"displayName":"Ben de Wet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjckrmJakAJs7u89oXMe8_3gnV92h_im7fLsR-CIg=s64","userId":"08305680562493196196"}},"outputId":"95916af1-abd8-4940-8d6c-075d1e9dc100"},"source":["for i in range(len(model.layers)):\n","    layer = model.layers[i]\n","    if 'conv' in layer.name:\n","      print(i , layer.name , layer.output.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1 conv2d_3 (None, 28, 28, 32)\n","4 conv2d_4 (None, 14, 14, 64)\n","7 conv2d_5 (None, 7, 7, 128)\n"]}]},{"cell_type":"code","metadata":{"id":"nWVPExm46yop"},"source":["drive.mount('/gdrive')\n","save_weights_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath='/gdrive/MyDrive/AI/weights',\n","    save_weights_only=True,\n","    verbose=1,\n","    save_freq=3*1875)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DJYiGAdoHqxB"},"source":["# Train Model"]},{"cell_type":"code","metadata":{"id":"JoSnwIiW_s-x"},"source":["\n","training_history = model.fit(training_images, \n","                             training_labels_categorical, \n","                             epochs=3, \n","                             validation_data=(testing_images, \n","                                              testing_labels_categorical))\n","                            #  callbacks=[save_weights_callback])\n","\n","plt.plot(training_history.history['accuracy'])\n","plt.plot(training_history.history['val_accuracy'])\n","plt.title('model accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'test'], loc='upper left')\n","plt.show()\n","\n","# summarize history for loss\n","plt.plot(training_history.history['loss'])\n","plt.plot(training_history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'test'], loc='upper left')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_TCSGBy6E2bv"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"37ldMd1FIA9d"},"source":["# Visualise Trained Model"]},{"cell_type":"code","metadata":{"id":"vZysjT2JIEja"},"source":["# get filter weights of the first convolutional layer\n","filters, biases = model.layers[1].get_weights()\n","\n","# normalize filter values to 0-1 so we can visualize them\n","f_min, f_max = filters.min(), filters.max()\n","filters = (filters - f_min) / (f_max - f_min)\n","\n","# 1st layer has 32 filters (figsize is in inches) https://www.pythonpool.com/matplotlib-figsize/\n","# TODO: Find out how to add title to the image\n","plt.figure(figsize=(20,2))\n","for i in range(32):\n","  ax = plt.subplot(2, 16, i+1)\n","  f = filters[:, :, :, i-1]\n","  plt.imshow(f[:, :, 0], cmap='binary')\n","  ax.set_xticks([])\n","  ax.set_yticks([])\n","\n","print('1st convolution layer')\n","plt.show()\n","\n","# get filter weights of the first convolutional layer\n","filters, biases = model.layers[4].get_weights()\n","\n","# normalize filter values to 0-1 so we can visualize them\n","f_min, f_max = filters.min(), filters.max()\n","filters = (filters - f_min) / (f_max - f_min)\n","\n","# display the second convolutional layer\n","plt.figure(figsize=(20,4))\n","for i in range(64):\n","  ax = plt.subplot(4, 16, i+1)\n","  f = filters[:, :, :, i-1]\n","  plt.imshow(f[:, :, 0], cmap='binary')\n","  ax.set_xticks([])\n","  ax.set_yticks([])\n","\n","print('2nd convolution layer')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NutvGKMkHuwP"},"source":["# Use model to predict"]},{"cell_type":"code","metadata":{"id":"hbifhwhJ_ur3"},"source":["prediction_count = 20\n","prediction_list = model.predict(testing_images[0:prediction_count])\n","x_img = testing_images.reshape((10000, 28, 28))\n","\n","for i in range(prediction_count):\n","  plt.subplot(3, 1, 1, frameon=False)\n","  plt.title(f'label={testing_labels[i]}, predict={np.argmax(prediction_list[i])}')\n","  plt.imshow(x_img[i], cmap = plt.get_cmap('binary'))\n","  plt.show()  \n","\n","#for sublist in prediction_list:\n","#  list_pos = np.where(prediction_list == sublist)[0][0]\n","#  print(f\"Prediction: {np.argmax(sublist)} Actual: {np.argmax(y_data[list_pos])}\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gOpauW9OH0QR"},"source":["# Save Model"]},{"cell_type":"code","metadata":{"id":"sGhkRExw_v68"},"source":["drive.mount('/gdrive')\n","reconstructed_model = model.save('/gdrive/MyDrive/model')"],"execution_count":null,"outputs":[]}]}