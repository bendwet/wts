{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"xS5tIQR4_FWn"},"outputs":[],"source":["import tensorflow as tf\n","import matplotlib.pyplot as plt\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing import image \n","from PIL import Image\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"K4jnyp6pHWNC"},"source":["# Load Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vh3aidjjPx4M"},"outputs":[],"source":["print(tf.__version__)\n","\n","print('Load model')\n","\n","from google.colab import drive\n","drive.mount('/gdrive')\n","model = tf.keras.models.load_model('/gdrive/MyDrive/improved_dropout_model/improved_dropout_model')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f2q_y02FQXHs"},"outputs":[],"source":["print('Load test image')\n","\n","    # open image\n","    image_open = Image.open('test_images/hand_four2.PNG')\n","    # sharpen image\n","    sharpen_img = ImageEnhance.Sharpness(image_open)\n","    sharpened_img = sharpen_img.enhance(1.0)\n","    # increase contrast of image\n","    contrast_img = ImageEnhance.Contrast(sharpened_img)\n","    contrasted_img = contrast_img.enhance(10.0)\n","    # convert image to rgb\n","    rgb_image = contrasted_img.convert('RGB')\n","    # convert image to array\n","    rgb_image_array = tf.keras.preprocessing.image.img_to_array(rgb_image)\n","\n","    # convert full sized image to grayscale\n","    gray_full_size_img = tf.image.rgb_to_grayscale(rgb_image_array)\n","    # squeeze to remove colour channel for plotting\n","    squeeze_gray_full_img = np.squeeze(gray_full_size_img)\n","    resized_img = tf.image.resize(gray_full_size_img, [28, 28])\n","    # squeeze resized grayscale image\n","    grayscale_image = np.squeeze(resized_img)\n","\n","    print('Contrasted image as loaded')\n","    plt.imshow(rgb_image_array, cmap='gray')\n","    plt.show()\n","\n","    print('gray scale image')\n","    plt.imshow(squeeze_gray_full_img, cmap='gray')\n","    plt.show()\n","\n","    print('gray scale resized image')\n","\n","    plt.imshow(grayscale_image, cmap='gray')\n","    plt.show()\n","\n","    # todo : detect if image is mostly white, and invert if required\n","    inverted_image = 255 - resized_img\n","    relu_layer = tf.keras.layers.ReLU(threshold=10)\n","    cleaned_inverted_image = relu_layer(inverted_image)\n","\n","    # normalize values to scale them between 0-1\n","    cleaned_inverted_image = cleaned_inverted_image / 255\n","\n","    # add batch size of 1 to image\n","    cleaned_inverted_image = tf.expand_dims(cleaned_inverted_image, axis=0)\n"]},{"cell_type":"markdown","metadata":{"id":"NutvGKMkHuwP"},"source":["# Use model to predict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hbifhwhJ_ur3"},"outputs":[],"source":["prediction_list = model.predict(cleaned_inverted_image)\n","# x_img = testing_images.reshape((10000, 28, 28))\n","print(prediction_list)\n","print(np.argmax(prediction_list[0]))\n","\n","\"\"\"for i in range(prediction_count):\n","  plt.subplot(3, 1, 1, frameon=False)\n","  plt.title(f'label={testing_labels[i]}, predict={np.argmax(prediction_list[i])}')\n","  plt.imshow(x_img[i], cmap = plt.get_cmap('binary'))\n","  plt.show()  \n","\"\"\"\n"]},{"cell_type":"markdown","metadata":{"id":"gOpauW9OH0QR"},"source":["# Save Model"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"cnnproject_runtime.ipynb","provenance":[{"file_id":"1wtCV1jSWZ67lFs3Ok8dyJozkGmAeVkzR","timestamp":1631411133772},{"file_id":"1smNf7QBX-JB3o6jTedgoAS1jIowEWjKZ","timestamp":1631319906433}],"toc_visible":true,"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}